import com.fasterxml.jackson.databind.ObjectMapper
import java.nio.file.Paths
import java.nio.file.Files

buildscript {
    repositories {
        mavenCentral()
        maven {
            url = uri("https://packages.confluent.io/maven/")
        }
        maven {
            url = uri("https://plugins.gradle.org/m2/")
        }
        maven {
            url = uri("https://jitpack.io")
        }
    }
}


plugins {
    id 'idea'
    id 'eclipse'
    id "com.google.protobuf" version "0.8.17"
    id "com.github.imflog.kafka-schema-registry-gradle-plugin" version "1.1.1"
    id "com.github.davidmc24.gradle.plugin.avro" version "1.0.0"
}

group 'io.confluent.developer'
version '1.0-SNAPSHOT'

sourceCompatibility = JavaVersion.VERSION_11
targetCompatibility = JavaVersion.VERSION_11

repositories {
    mavenCentral()
    maven {
        url = uri("https://packages.confluent.io/maven/")
    }

    maven {
        url = uri("https://jitpack.io")
    }
}

dependencies {
    implementation platform('software.amazon.awssdk:bom:2.10.73')
    implementation 'software.amazon.awssdk:lambda:2.17.24'
    implementation 'org.apache.kafka:kafka-clients:2.8.0'
    implementation 'com.amazonaws:aws-lambda-java-core:1.2.1'
    implementation 'com.amazonaws:aws-lambda-java-events:3.9.0'
    implementation 'software.amazon.awssdk:secretsmanager:2.17.24'
    implementation 'com.fasterxml.jackson.core:jackson-databind:2.12.4'
    implementation 'com.google.code.gson:gson:2.8.8'
    implementation 'org.apache.logging.log4j:log4j-api:2.15.0'
    implementation 'org.apache.logging.log4j:log4j-core:2.15.0'

    implementation 'com.google.protobuf:protobuf-java:3.17.3'
    implementation 'org.apache.avro:avro:1.10.2'
    implementation('org.apache.kafka:kafka-clients:2.8.0!!')
    implementation('io.confluent:kafka-streams-avro-serde:6.1.1') {
        exclude group: 'org.apache.kafka', module: 'kafka-clients'
        exclude group: 'org.apache.kafka', module: 'kafka-streams'
    }
    implementation('io.confluent:kafka-streams-protobuf-serde:6.1.1') {
        exclude group: 'org.apache.kafka', module: 'kafka-clients'
        exclude group: 'org.apache.kafka', module: 'kafka-streams'
    }
    implementation "io.confluent:kafka-avro-serializer:6.1.1"
    implementation "io.confluent:kafka-protobuf-serializer:6.1.1"
    implementation "io.confluent:kafka-protobuf-provider:6.1.1"


    runtimeOnly 'org.apache.logging.log4j:log4j-slf4j18-impl:2.15.0'
    runtimeOnly 'com.amazonaws:aws-lambda-java-log4j2:1.3.0'
    testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.2'
    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.7.2'
}

protobuf {
    generatedFilesBaseDir = "${project.buildDir}/generated-main-proto-java"

    protoc {
        artifact = 'com.google.protobuf:protoc:3.15.3'
    }
}

/*
  This task looks for a file named 'confluent.properties' in the
  src/main/resources directory and creates a JSON file 'aws-cli/aws-ccloud-creds.json'
  and is used by both the create-secret and update-secret scripts for safely
  storing your CCloud connection credentials
 */

class PropertiesToJsonTask extends DefaultTask {
    @TaskAction
    def propsToJson() {
        Map<String, String> baseDatagenConfigs = ["connector.class":"DatagenSource", "output.data.format":"JSON", "tasks.max":"1"]
        ObjectMapper objectMapper = new ObjectMapper();
        Properties properties = new Properties();
        // We do this to always grab the latest java-service-configs file from the stack-configs directory if previous ones aren't deleted
        File stackConfigs = Files.list(Paths.get("stack-configs"))
                .sorted((path1, path2) -> path2.toFile().lastModified() <=> path1.toFile().lastModified())
                .findFirst().get().toFile()

        try (FileInputStream fis = new FileInputStream(stackConfigs)) {
            properties.load(fis);
        }
        // Taking these out because they aren't sensitive values plus we want the same entries between different deployments
        ["security.protocol", "sasl.mechanism", "client.dns.lookup",
         "acks", "basic.auth.credentials.source", "replication.factor"].forEach(k -> properties.remove(k))
        String saslConfig = properties.get("sasl.jaas.config")
        // We're doing this because the AWS security manager wants a separate username and password entry for configuring the event source
        def matcher = saslConfig =~ /.*username='([^']*)'.*password='([^']*)'.*/
        if (matcher.find()) {
            String username = matcher.group(1)
            String password = matcher.group(2)
            properties.put("username", username)
            properties.put("password", password)
            baseDatagenConfigs.put("kafka.api.key", username)
            baseDatagenConfigs.put("kafka.api.secret", password)
        } else {
            throw new RuntimeException("AWS security manager requires 'username' and 'password' entries ")
        }

        Map<String, String> stocktradeConfigs = new HashMap<>(baseDatagenConfigs)
        stocktradeConfigs.put("quickstart", "STOCK_TRADES")
        stocktradeConfigs.put("kafka.topic", "stocktrade")
        stocktradeConfigs.put("name","StockTradeDatagen")
        Map<String, String> userConfigs = new HashMap<>(baseDatagenConfigs)
        userConfigs.put("quickstart", "USERS")
        userConfigs.put("kafka.topic", "users")
        userConfigs.put("name", "UsersDatagen")

        File destinationJsonFile = new File("aws-cli/aws-ccloud-creds.json")
        File stocktradeDatagenJsonFile = new File("src/main/resources/stocktrade-datagen.json")
        File userDatagenJsonFile = new File("src/main/resources/user-datagen.json")
        
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(destinationJsonFile, properties)
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(stocktradeDatagenJsonFile, stocktradeConfigs)
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(userDatagenJsonFile, userConfigs)
    }
}

tasks.register("propsToJson", PropertiesToJsonTask)



task buildZip(type: Zip) {
    from compileJava
    from processResources
    into('lib') {
        from configurations.runtimeClasspath
    }
}
